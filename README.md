# Language-Agnostic-Transformers-Analysis


**Q1.** For a downstream task like text classification, is it better to use a monolingual model trained on a single language or a multilingual model exposed to dozens of languages?  
**Notebook:** `zero_shot_analysis.ipynb`

**Q2.** Given that simpler multilingual models remain competitive for classification, how do we pick the best pretrained model?  
**Notebook:** `Embedding_Similarity_Testing.ipynb` implements a tookkit to evaluate embedding quality for downstream classification.
